package com.example.bigdata
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{regexp_extract, udf}

object pm25 {
  def main(args: Array[String]): Unit = {
    val username = System.getProperty("user.name")

    val spark: SparkSession = SparkSession.builder
      .appName("test")
      .master("local[2]")
      .getOrCreate()

    val df = spark.read.option("sep", ";").csv(s"/user/$username/labs/spark/externaldata/london-air-quality.txt");

    val pattern1: String = """...\-..""";
    val pattern2: String = """(?<=PM2\.5 Particulate: )[1-9\.]*""";

    val df2 = df.withColumn("date", regexp_extract(df("_c0"), pattern1, 0))
      .withColumn("pm25", regexp_extract(df("_c0"), pattern2, 0));

    val pm25_coder = udf((pm25_value:Float) => {
      if (pm25_value < 12.0) 1
      else if (pm25_value < 24.0) 2
      else if (pm25_value < 36.0) 3
      else if (pm25_value < 42.0) 4
      else if (pm25_value < 48.0) 5
      else if (pm25_value < 54.0) 6
      else if (pm25_value < 59.0) 7
      else if (pm25_value < 65.0) 8
      else if (pm25_value < 71.0) 9
      else 10
    })

    val df3 = df2.withColumn("pm25_category", pm25_coder(df2("pm25")))
      .withColumn("pm25TMP", df2("pm25").cast("float"))
      .drop("pm25")
      .withColumnRenamed("pm25TMP", "pm25");
  }
}
package com.example.bigdata
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.catalyst.dsl.expressions.StringToAttributeConversionHelper
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.row_number

object crime_category {
  def main(args: Array[String]) {
    val username = System.getProperty("user.name")

    val spark: SparkSession = SparkSession.builder
      .appName("test")
      .master("local[2]")
      .getOrCreate()

    val colp_DF = spark.read.format("org.apache.spark.csv").
      option("header", true).option("inferSchema", true).
      csv(s"/user/$username/labs/spark/externaldata/CoLP-london-crimes.csv").cache();

    val mp_DF = spark.read.format("org.apache.spark.csv").
      option("header", true).option("inferSchema", true).
      csv(s"/user/$username/labs/spark/externaldata/MP-london-crimes.csv").cache();

    val all_crimes_DF = colp_DF.
      union(mp_DF);


    all_crimes_DF.select("major_category", "minor_category").distinct().withColumn("index", row_number().over(Window.orderBy(all_crimes_DF("major_category"), all_crimes_DF("minor_category"))))
      .select("index", "major_category", "minor_category").
      write.insertInto("w_crime_category");
  }
}
